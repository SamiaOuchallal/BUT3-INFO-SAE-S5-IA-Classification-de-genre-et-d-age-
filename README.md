# Projet : Classification d‚Äô√¢ge et de genre en utilisant des r√©seaux de neurones convolutionnels (CNN)

# üìÑ Mise en contexte
Ce projet a pour objectif de d√©velopper une application capable de pr√©dire le genre et l‚Äô√¢ge √† partir d‚Äôune image de visage. 

### L‚Äôobjectif de ce projet va √™tre de r√©aliser 4 mod√®les diff√©rents en utilisant le jeu de donn√©es UTKFace, √† savoir :  
* Mod√®le de Classification de Genre avec CNN 
* Mod√®le de Classification d‚Äô√¢ge avec une approche de r√©gression 
* Mod√®le de Classification simultan√©e de Genre et d‚Äô√¢ge 
* Mod√®le pr√©-entra√Æn√© avec l‚Äôutilisation du transfer learning et comparatifs avec d‚Äôautres mod√®les pr√©-entra√Æn√©s

### Pour permettre par la suite de :
* Cr√©er une interface Gradio
* D√©ployer l'application et tous les mod√®les sur HuggingFaces

## üë• Membres de l'√©quipe
* CABO India
* GIFFARD Axel
* HAMSEK Fay√ßal
* OUCHALLAL Samia

La classification de l'√¢ge et du genre est un processus permettant de d√©tecter l'√¢ge et le genre (Homme/Femme) d'une personne en se basant sur des caract√©ristiques de son visage. On consid√®re les caract√©ristiques d'une personne par ses traits de visage, ses imperfections, sa pilosit√©, les rides,etc... \
Toutes ces caract√©ristiques am√®nes √† d√©tecter l'√¢ge ou le genre d'une personne en fonction d'algorithmes de DeepLearning. Cependant, bien que la d√©tection soit possible, **elle n‚Äôen est pas moins certifi√©e v√©ridique tout le temps.** L'estimation de l'√¢ge varie selon **plusieurs facteurs**, tels que les lumi√®res de l'image, les expressions faciales, le maquillage pour rendre la peau plus "jeune", ...

# Pourquoi r√©aliser ce projet ? 
<details>
<summary><b>D√©roulez pour voir l'ensemble des objectifs : 
</b></summary><br/>
  
- **Exploration et pr√©paration des donn√©es** \
  *Analyse du dataset UTKFace (distribution des √¢ges, √©quilibre hommes/femmes)
  *Pr√©traitement des images (normalisation, redimensionnement)
  *Augmentation de donn√©es pour am√©liorer la robustesse

- **Comprendre et appliquer les techniques propres au DeepLearning**  
Cela implique d'avoir des notions en math√©matiques, science des donn√©es, et informatiques pour appliquer des algorithmes d'optimisation lin√©aire (Fonctions d'activation), de savoir et conna√Ætre l'ensemble des param√®tres et hyperparam√®tres utilis√©s, et de savoir optimiser nos mod√®les en utilisant des techniques (Dropout, BatchNormalization, learning rate, ...).

- **Comparer diff√©rentes architectures de CNN** \
√âvaluer les avantages et inconv√©nients de diff√©rentes architectures pour ces t√¢ches sp√©cifiques.

- **Analyser les biais potentiels** \
Identifier les biais potentiels dans les pr√©dictions selon l'√©clairage, la qualit√© de l'image, etc. Les mod√®les doivent √™tre robustes pour performer de mani√®re constante.

- **D√©velopper une interface utilisateur** \
Cr√©er une interface simple permettant de tester les mod√®les sur de nouvelles images avec Gradio.
Cela permettra au cours de nos √©tudes de pr√©senter ce projet et que les utilisateurs puissent tester l'ensemble de nos mod√®les.

</details>
  
## üõ†Ô∏è Langages et outils
- [Python](https://docs.python.org/)
- [Tensorflow](https://www.tensorflow.org/api_docs).
- [Keras](https://keras.io/).
- [Gradio](https://www.gradio.app/docs).
- [HuggingFaces](https://huggingface.co/)
- [Pandas](https://pandas.pydata.org/docs/)
- [Matplotlib](https://matplotlib.org/stable/index.html)
- [Seaborn](https://seaborn.pydata.org/)


## R√©sum√© de nos mod√®les 

| **Mod√®le**                                                                                | **R√©sum√©**                                                                                                                                                                        | **Liens**                                                    | **M√©triques**                          |
|-------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------|
| Mod√®le 1 (Genre)                                                                                 | Classification du genre de mani√®re binaire                                                                    | https://github.com/lxq1000/SwinFace                         | Binary_accuracy, F1_score, AUC                              |
| Mod√®le 2 (Age)                                                      | R√©gression de l'√¢ge                                                                                                                                                    | https://github.com/paplhjak/facial-age-estimation-benchmark | MAE,MSE                                 |
| Mod√®le 3 (Genre + Age)                                                                             | Classification de genre et r√©gression de l'√¢ge en les combinant simultan√©ment                                                                                                                           | https://github.com/WildChlamydia/MiVOLO                     | MAE,MSE, Binary_accuracy                 |
| Mod√®le 4 (Transfer Learning)                                                                           | Comparatifs de mod√®les pr√©-entra√Æn√©s (EfficientNetB0/1/2, VGG16, MobileNetV2)                                                                                                                           | https://github.com/WildChlamydia/MiVOLO                     | MAE,MSE, RMSE, Precision, Recall, F1_score, Accuracy                 |

R√©alis√© avec https://www.tablesgenerator.com/markdown_tables
</details>


# üñºÔ∏è Le dataset UTKFace
   ## UTKFace
   Le dataset est [UTKFace](https://susanqq.github.io/UTKFace/). C‚Äôest un dataset compos√© de 23708 images avec toutes les ethniques, l'ensemble des genres et de l'√¢ge allant de 0 √† 116 ans. Ces images peuvent avoir des tons de couleurs diff√©rents, 
  et des variations dans l‚Äôexpression des visages.
  
  ![alt text](images/Personnes.png)  


## Distribution du genre 

![alt text](images/distribGenre.png)

On rep√®re 52.3 % d'hommes et 47.7 % de femmes.
**Cette distribution indique un potentiel biais concernant la classe minoritaire (femmes) et la classe majoritaire (hommes). Les mod√®les seront susceptibles de se baser sur la classe majoritaire durant l'entra√Ænement, ce qui peut conduire √† un surapprentissage (overfitting). Or, ce biais reste n√©anmoins faible et ne devrait pas poser d'importants probl√®mes dans les r√©sultats car une diff√©rence de 4 % peut √™tre consid√©r√© comme quasi-√©quilibr√©e.

## Distribution de l'√¢ge 

![alt text](images/DistributionAge.png)

En faisant cette visualisation, nous remarquons qu'il y a un fort d√©s√©quilibre entre les diff√©rents √¢ges. Par exemple, il y a √©norm√©ment d‚Äôimages de personnes qui ont un √¢ge proche de 26 √† 40 ans, peu de jeunes et encore moins de personnes √¢g√©es autour de 70 ans. En faisant la moyenne, nous en avons trouv√© que le taux le plus important en termes d‚Äô√¢ge √©tait de 33 ans.
**En cons√©quence, le mod√®le pourrait √™tre plus performant pour estimer l‚Äô√¢ge des personnes ayant entre 20 et 40 ans que pour estimer l‚Äô√¢ge des personnes entre 60 et 116 ans.**

### ‚úîÔ∏è T√¢ches pour optimiser les mod√®les : 
* Normaliser le genre et l'√¢ge
* R√©aliser de la Data Augmentation
* Tranche d'√¢ge pour le mod√®le de l'√¢ge
* Ajuster et exp√©rimenter les param√®tres et hyperparam√®tres
* Exp√©rimenter les tailles de Batch

## Pr√©paration des donn√©es 
Nous faisons un "split" des donn√©es gr√¢ce √† la m√©thode train_test_split de la librairie sklearn
`
    x_train, x_test, y_train, y_test = train_test_split(
        df['image'],
        df['gender_encoded']],
        test_size=0.2,
        random_state=42
    )
    # Split suppl√©mentaire pour validation
    x_train, x_val, y_train, y_val = train_test_split(
        x_train,
        y_train,
        test_size=0.2, 
        random_state=42
    )
` 

| Dataset      	| Donn√©es 	|
|--------------	|---------	|
| Entra√Ænement 	| 18966   	|
| Validation   	| 4742    	|

Nous divisons le dataset en train + val avec 80 % pour le train et 20 % pour le val

## Pr√©-traitement des donn√©es

Les images sont redimenssionn√©es √† une taille uniforme : **224x224** pour les mod√®les pr√©-entra√Æn√©s (conventions de ces mod√®les) et **128x128** pour le reste des mod√®les.

### Normalisation de l'√¢ge et du genre

* On vient normaliser l'√¢ge en prenant en compte l'√¢ge suppos√© maximale dans le dataset,
```python
normalized_age = tf.cast(age,tf.float32) / 116.0
```
* On vient normaliser le genre en divisant par 255 pour transformer la valeur des pixels entre 0 et 1. Ces valeurs vont s'adapter plus rapidement lors de l'entra√Ænement
  ```python
  image=image/255.0
  ```

### Data Augmentation
La technique de la Data Augmentation va permettre d'augmenter la taille du dataset : En cr√©ant de nouvelles images √† partir des images existantes, on multiplie artificiellement la quantit√© de donn√©es d'entra√Ænement disponibles.
Elle va permettre √©galement de r√©duire le risque d'overfitting en l'emp√™chant de se baser sur les m√™mes visages.
Elle inclue des transformations de type : 
* Rotation
* Flip
* Zoom
* Luminosit√©/contraste/saturation

```python
image = tf.image.random_crop(image, size=(96, 96, 3))
        image = tf.image.resize(image, size=size)
        image = tf.image.random_flip_left_right(image)
        image = tf.image.random_brightness(image, max_delta=0.2)
```


</br>
Ce qui peut donner pour un set d'images comme celui-ci : 

![image](https://github.com/user-attachments/assets/989e433c-5e9f-4831-84fd-d343160c7999)


## üìÑ Hyperparam√®tres utilis√©es
* Type d'activation : (relu, Sigmoid,softmax)
* learning rate : G√©n√©ralement 0.0001
* Taille du batch : G√©n√©ralement 32
* Optimiseur : G√©n√©ralement Adam
* Dropout : Test√© sur 0.2, 0.3 et 0.5
* Kernel_regularizer : G√©n√©ralement 0.0005
* kernel_size
* strides
* pooling_size=G√©n√©ralement MaxPooling

### Mod√®les pr√©-entra√Æn√©s + Graphiques
Chaque image est en dimension 224x224

<details>
  <summary><b>EfficientNet (B0/B1/B2)</b></summary><br/>
  
  ![ image](images/EfficientNet-Architecture-diagram.png)

  # EfficientNetB0 :
  ```python
  base_model_efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))
  ```
![image](images/EfficietNetB0Courbe.png)

 ### R√©sultats (50 epochs)

  # EfficientNetB1 :
  ```python
  base_model_efficientnet = EfficientNetB1(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))
  ```
 ### R√©sultats (50 epochs)

  # EfficientNetB2 :  
  ```python
  base_model_efficientnet = EfficientNetB2(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))
  ```

 ### R√©sultats (50 epochs)
  ![image](images/EfficientNetB2Courbes.png)


</details>
<details>
  <summary><b>VGG16</b></summary><br/>
  
  ![image](images/VGG16.png)
  
   ### R√©sultats (50 epochs)

  ![ image](images/images/VGG16Courbes.png)

</details>

<details>
  <summary><b>MobileNetV2 </b></summary><br/>
  
  ![image](images/images/539617_1_En_8_Fig10_HTML.png)
  
   ### R√©sultats (50 epochs)
   
  ![image](images/images/images/MobileNetV2Courbes.png)

</details>

<details>
  <summary><b>ResNet50</b></summary><br/>
  
  ![image](images/images/images/res.png)

</details>

| Mod√®les             | Accuracy - Genre  | AUC | F1_Score | Precision | Recall | MAE  | MSE    | RMSE  | Age Accuracy (10 ans d'√©cart en %) | #Params |
|---------------------|-------------------|-----|----------|-----------|--------|------|--------|-------|------------------------------------|---------|
| Genre               |        81.66 %    |0.90 |   0.82   |    0.82   |  0.82  |  -   | -     | -    | -                                    |         |
|         Age         |         -         |  -  |     -    |     -     |    -   | 5.81 |  63.45 |  9.24 |                                    | 423,361 |
| Genre + Age         |        0.90       |  -  |          |           |        | 5.81 |  63.45 |  9.24 |                                    |8,779,970|
| TA - EfficientNetB2 |        0.9373     |  -  |   0.94   |    0.95   |  0.93  | 5.52 |  62.79 |  7.92 |               83.64%               |         |
| TA - EfficientNetB0 |        0.90       |  -  |   0.90   |    0.90   |  0.81  |   7  |  96.95 |  9.85 |               75.96%               |         |
| TA - EfficientNetB1 |        0.90       |  -  |   0.91   |    0.90   |  0.91  | 6.98 | 103.55 | 10.18 |               76.06%               |         |
|      TA - VGG16     |        0.88       |  -  |   0.89   |    0.89   |  0.89  | 6.86 | 134.40 | 11.59 |               69.57%               |         |
|   TA - MobileNetV2  |        0.93       |  -  |   0.93   |    0.93   |  0.93  | 6.07 |  76.52 |  8.75 |                80.64               |   3.2M  |

En r√©sum√©, le meilleur mod√®le pr√©-entrain√© est EfficientNetB2 qui prime avec 93 % d'accuracy pour le genre, avec 5.52 d'MAE. Autrement dit, le mod√®le peut se tromper de genre avec une probabilit√© de 7%, tandis que pour l'√¢ge, le mod√®le est susceptible de se tromper entre 5 et 6 ans d'√©cart. Il peut aussi √™tre ammen√© √† une probabilit√© de se rapprocher de 83.64 % entre 0 et jusqu'√† 10 ans de plus. 


## ‚ú® Demos
- [Hugging Face Spaces Application Demo ](https://huggingface.co/spaces/samsam2908/ia_sae)
